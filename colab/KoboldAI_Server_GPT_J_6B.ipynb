{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KoboldAI Server - GPT-J-6B.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HflRYUWt8jo"
      },
      "source": [
        "**Welcome to the KoboldAI Colab Service GPT-J-6B Notebook!**<br/>\n",
        "*Note: This colab is intended to be used with the KoboldAI Client, [which can be downloaded from GitHub here](https://github.com/KoboldAI/KoboldAI-Client).*</br>\n",
        "**>> This notebook has been superseded by [the GPT-J-6B Rev 2 Notebook](https://colab.research.google.com/drive/1VFh5DOkCJjWIrQ6eB82lxGKKPgXmsO5D?usp=sharing)! <<**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5NcA61O-S02",
        "cellView": "form"
      },
      "source": [
        "#@title <b>Step 1 - Install Dependencies</b>\n",
        "#@markdown Press the Play button and wait for the script to finish.\n",
        "from IPython.display import clear_output\n",
        "from termcolor import colored\n",
        "import os\n",
        "\n",
        "!pip install flask-ngrok\n",
        "!pip install termcolor\n",
        "!pip install flask_cloudflared\n",
        "!pip install optax\n",
        "!apt install zstd\n",
        "if not os.path.isdir(\"step_383500\"):\n",
        "   !time wget https://the-eye.eu/public/AI/GPT-J-6B/step_383500_slim.tar.zstd\n",
        "   !time tar -I zstd -xf step_383500_slim.tar.zstd\n",
        "!git clone https://github.com/kingoflolz/mesh-transformer-jax.git\n",
        "!pip install -r mesh-transformer-jax/requirements.txt\n",
        "!pip install mesh-transformer-jax/ jax==0.2.12\n",
        "clear_output()\n",
        "print(colored(\"Installing DONE!\", \"green\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUpeliUiaUi3",
        "cellView": "form"
      },
      "source": [
        "#@title <b>Step 2 - Adjust Your Settings</b>\n",
        "#@markdown 1. Connect via Ngrok or Cloudflare?\n",
        "connect_method = \"Ngrok\" #@param [\"Ngrok\", \"Cloudflare\"]\n",
        "#@markdown 2. Press Play button to lock in settings <b>(Do not skip!)</b>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aGzVuDR_eo1"
      },
      "source": [
        "#@title <b>Step 3 - Initialize Model</b> { display-mode: \"form\" }\n",
        "#@markdown Press the Play button. Wait for the model to complete\n",
        "#@markdown initialization. This can take 5+ minutes.</br>\n",
        "#@markdown When the word DONE! is displayed, you can move on to\n",
        "#@markdown the next Step.</br></br>\n",
        "#@markdown <b>>> If you get an error when running this cell, run it again! <<</b>\n",
        "\n",
        "from flask import Flask, redirect, url_for, request\n",
        "import json\n",
        "import torch\n",
        "import requests\n",
        "import subprocess\n",
        "import tarfile\n",
        "from jax.config import config\n",
        "import time\n",
        "\n",
        "# Sometimes the next step errors for some reason, just run it again\n",
        "import jax\n",
        "from jax.experimental import maps\n",
        "import numpy as np\n",
        "import optax\n",
        "import transformers\n",
        "from mesh_transformer.checkpoint import read_ckpt\n",
        "from mesh_transformer.sampling import nucleaus_sample\n",
        "from mesh_transformer.transformer_shard import CausalTransformer\n",
        "\n",
        "# Initialize the model\n",
        "print(colored(\"Initializing model, please wait...\", \"magenta\"))\n",
        "\n",
        "colab_tpu_addr = os.environ['COLAB_TPU_ADDR'].split(':')[0]\n",
        "url = f'http://{colab_tpu_addr}:8475/requestversion/tpu_driver0.1_dev20210607'\n",
        "requests.post(url)\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "\n",
        "params = {\n",
        "  \"layers\": 28,\n",
        "  \"d_model\": 4096,\n",
        "  \"n_heads\": 16,\n",
        "  \"n_vocab\": 50400,\n",
        "  \"norm\": \"layernorm\",\n",
        "  \"pe\": \"rotary\",\n",
        "  \"pe_rotary_dims\": 64,\n",
        "\n",
        "  \"seq\": 2048,\n",
        "  \"cores_per_replica\": 8,\n",
        "  \"per_replica_batch\": 1,\n",
        "}\n",
        "\n",
        "per_replica_batch = params[\"per_replica_batch\"]\n",
        "cores_per_replica = params[\"cores_per_replica\"]\n",
        "seq = params[\"seq\"]\n",
        "\n",
        "\n",
        "params[\"sampler\"] = nucleaus_sample\n",
        "params[\"optimizer\"] = optax.scale(0)\n",
        "\n",
        "mesh_shape = (jax.device_count() // cores_per_replica, cores_per_replica)\n",
        "devices = np.array(jax.devices()).reshape(mesh_shape)\n",
        "\n",
        "maps.thread_resources.env = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))\n",
        "tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')\n",
        "total_batch = per_replica_batch * jax.device_count() // cores_per_replica\n",
        "print(colored(\"Creating CasualTransformer instance...\", \"magenta\"))\n",
        "network = CausalTransformer(params)\n",
        "print(colored(\"Reading checkpoint...\", \"magenta\"))\n",
        "network.state = read_ckpt(network.state, \"step_383500/\", devices.shape[1])\n",
        "print(colored(\"Calling move_xmap...\", \"magenta\"))\n",
        "network.state = network.move_xmap(network.state, np.zeros(cores_per_replica))\n",
        "\n",
        "clear_output()\n",
        "print(colored(\"DONE!\", \"green\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spRcqOxM3CqX"
      },
      "source": [
        "#@title <b>Step 4 - Run Web Service</b> { display-mode: \"form\" }\n",
        "#@markdown Press the Play button. Flask will start and give you a \n",
        "#@markdown Cloudflare or Ngrok address which looks like this:<br/>\n",
        "#@markdown <i>https://\\<unique id\\>.trycloudflare.com/</i><br/>\n",
        "#@markdown <i>http://\\<unique id\\>.ngrok.io/</i><br/>\n",
        "#@markdown You will need to right-click this and copy the address.\n",
        "#@markdown Start the KoboldAI Client on your computer and choose \n",
        "#@markdown Google Colab as the model. You will be asked to paste \n",
        "#@markdown the address into the terminal.<br/><br/>\n",
        "#@markdown If your session is interrupted, you can just restart\n",
        "#@markdown this cell to get a new address without reinitializing\n",
        "#@markdown the model.</br></br>\n",
        "#@markdown <b>The first generation takes around a minute due to \n",
        "#@markdown compilation, but after that it should only take about \n",
        "#@markdown 10 seconds per sample.</b>\n",
        "\n",
        "tenv = maps.ResourceEnv(maps.Mesh(devices, ('dp', 'mp')))\n",
        "\n",
        "if connect_method == \"Cloudflare\":\n",
        "   from flask_cloudflared import run_with_cloudflared\n",
        "elif connect_method == \"Ngrok\":\n",
        "   from flask_ngrok import run_with_ngrok\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "if connect_method == \"Cloudflare\":\n",
        "   run_with_cloudflared(app)\n",
        "elif connect_method == \"Ngrok\":\n",
        "   run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"<h1>KoboldAI Colab Service Running!</h1>\"\n",
        "\n",
        "@app.route('/request',methods = ['POST'])\n",
        "def koboldrequest():\n",
        "   if request.method == 'POST':\n",
        "      try:\n",
        "        clear_output()\n",
        "        js      = request.json\n",
        "        txt     = js[\"text\"]\n",
        "        min     = js[\"min\"]\n",
        "        max     = js[\"max\"]\n",
        "        rep_pen = js[\"rep_pen\"]\n",
        "        temp    = js[\"temperature\"]\n",
        "        top_p   = js[\"top_p\"]\n",
        "\n",
        "        gen_len = max - (min - 1)\n",
        "\n",
        "        print(colored(\"Received Data: {0}\".format(txt), \"yellow\"))\n",
        "        print(colored(\"Generating text, please wait...\", \"green\"))\n",
        "\n",
        "        # env has to be redefined with each call for some reason, else a threading error is produced\n",
        "        maps.thread_resources.env = tenv\n",
        "        \n",
        "        tokens = tokenizer.encode(txt)\n",
        "        provided_ctx = len(tokens)\n",
        "        pad_amount = seq - provided_ctx\n",
        "        padded_tokens = np.pad(tokens, ((pad_amount, 0),)).astype(np.uint32)\n",
        "        batched_tokens = np.array([padded_tokens] * total_batch)\n",
        "        length = np.ones(total_batch, dtype=np.uint32) * len(tokens)\n",
        "        output = network.generate(batched_tokens, length, gen_len, {\"top_p\": np.ones(total_batch) * top_p, \"temp\": np.ones(total_batch) * temp})\n",
        "        samples = []\n",
        "        decoded_tokens = output[1][0]\n",
        "\n",
        "        for o in decoded_tokens[:, :, 0]:\n",
        "          samples.append(tokenizer.decode(o))\n",
        "\n",
        "        genout = samples[0]\n",
        "\n",
        "        print(colored(\"Generated Text: {0}\".format(genout), \"cyan\"))\n",
        "        response = app.response_class(\n",
        "           response=json.dumps({\"data\": {\"seqs\": [genout]}}),\n",
        "           status=200,\n",
        "           mimetype='application/json'\n",
        "        )\n",
        "        \n",
        "        js         = {}\n",
        "        genout     = \"\"\n",
        "        \n",
        "        return response\n",
        "\n",
        "      except Exception as e:\n",
        "        print(colored(\"[ERROR] Something went wrong during generation!\", \"red\"))\n",
        "        print(colored(\"{0}\".format(e), \"red\"))\n",
        "        response = app.response_class(\n",
        "          response=json.dumps({\"error\": {\"extensions\": {\"code\": \"Something went wrong during generation! {0}\".format(e)}}}),\n",
        "          status=400,\n",
        "          mimetype='application/json'\n",
        "        )\n",
        "\n",
        "print(colored(\"Starup complete! Running web service.\", \"green\"))\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}