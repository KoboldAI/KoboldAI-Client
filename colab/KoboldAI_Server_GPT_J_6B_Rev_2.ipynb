{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KoboldAI Server - GPT-J-6B Rev 2.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HflRYUWt8jo"
      },
      "source": [
        "**Welcome to the KoboldAI Colab Service  GPT-J-6B Rev 2 Notebook!**<br/>\n",
        "*Note: This colab is intended to be used with the KoboldAI Client, [which can be downloaded from GitHub here](https://github.com/KoboldAI/KoboldAI-Client).*\n",
        "\n",
        "**Things you will need:**<br/>\n",
        "- A Google Drive account\n",
        "- A copy of the Torch-converted [GPT-J-6B checkpoint from here](https://drive.google.com/file/d/1NXP75l1Xa5s9K18yf3qLoZcR6p4Wced1/view?usp=sharing)\n",
        "\n",
        "**Preparation:**<br/>\n",
        "Click the link to the checkpoint archive and make a shortcut to it using the \"Add shortcut to Drive\" button in the top right corner."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5NcA61O-S02",
        "cellView": "form"
      },
      "source": [
        "#@title <b>Step 1 - Install Dependencies</b>\n",
        "#@markdown Press the Play button and wait for the script to finish.\n",
        "from IPython.display import clear_output\n",
        "from termcolor import colored\n",
        "\n",
        "!pip install flask-ngrok\n",
        "!pip install git+https://github.com/finetuneanon/transformers@gpt-neo-localattention3\n",
        "!pip install termcolor\n",
        "!pip install flask_cloudflared\n",
        "clear_output()\n",
        "print(colored(\"Installing DONE!\", \"green\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUpeliUiaUi3",
        "cellView": "form"
      },
      "source": [
        "#@title <b>Step 2 - Adjust Your Settings</b>\n",
        "#@markdown 1. Connect via Ngrok or Cloudflare?\n",
        "connect_method = \"Ngrok\" #@param [\"Ngrok\", \"Cloudflare\"]\n",
        "#@markdown 2. Press Play button to lock in settings <b>(Do not skip!)</b>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aGzVuDR_eo1"
      },
      "source": [
        "#@title <b>Step 3 - Initialize Model</b> { display-mode: \"form\" }\n",
        "#@markdown Press the Play button. You will be asked to link your \n",
        "#@markdown Google Drive account to this colab. Click the link that \n",
        "#@markdown pops up, give Drive permission at the prompt, then copy \n",
        "#@markdown the access code that's presented back into this notebook.\n",
        "#@markdown <br/><br/>\n",
        "#@markdown This notebook will then extract the tar file and initialize \n",
        "#@markdown the AI model. <b>This will take several minutes.</b>\n",
        "#@markdown When the word DONE! is displayed, you can move on to\n",
        "#@markdown the next Step.\n",
        "\n",
        "from flask import Flask, redirect, url_for, request\n",
        "import json\n",
        "import torch\n",
        "import requests\n",
        "import subprocess\n",
        "from transformers import AutoConfig, GPTNeoForCausalLM, AutoTokenizer\n",
        "import tarfile\n",
        "from google.colab import drive\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from threading import Timer\n",
        "\n",
        "if connect_method == \"Cloudflare\":\n",
        "   from flask_cloudflared import run_with_cloudflared\n",
        "elif connect_method == \"Ngrok\":\n",
        "   from flask_ngrok import run_with_ngrok\n",
        "\n",
        "model         = None\n",
        "tokenizer     = None\n",
        "\n",
        "# Get connected to Google Drive\n",
        "print(colored(\"Requesting Google Drive access...\", \"magenta\"))\n",
        "drive.mount('/content/drive/')\n",
        "# Set path to tar file and unpack it\n",
        "model_gdrive = \"/content/drive/MyDrive/j6b_ckpt.tar\"\n",
        "if not os.path.isdir(\"j6b_ckpt\"):\n",
        "    print(colored(\"Unpacking tar file, please wait...\", \"magenta\"))\n",
        "    tar = tarfile.open(model_gdrive, \"r\")\n",
        "    tar.extractall()\n",
        "    tar.close()\n",
        "\n",
        "# Initialize the model\n",
        "print(colored(\"Initializing model, please wait...\", \"magenta\"))\n",
        "config = AutoConfig.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
        "config.attention_layers = [\"global\"] * 28\n",
        "config.attention_types = [[\"global\"], 28]\n",
        "config.num_layers = 28\n",
        "config.num_heads = 16\n",
        "config.hidden_size = 256 * config.num_heads\n",
        "config.vocab_size = 50400\n",
        "config.rotary = True\n",
        "config.rotary_dim = 64\n",
        "config.jax = True\n",
        "\n",
        "try:\n",
        "    from collections.abc import MutableMapping\n",
        "except ImportError:\n",
        "    from collections import MutableMapping\n",
        "\n",
        "class Checkpoint(MutableMapping):\n",
        "    def __init__(self):\n",
        "        self.checkpoint = torch.load(\"j6b_ckpt/m.pt\", map_location=\"cpu\")\n",
        "    def __len__(self):\n",
        "        return len(self.checkpoint)\n",
        "    def __getitem__(self, key):\n",
        "        return torch.load(self.checkpoint[key], map_location=\"cpu\")\n",
        "    def __setitem__(self, key, value):\n",
        "        return\n",
        "    def __delitem__(self, key, value):\n",
        "        return\n",
        "    def keys(self):\n",
        "        return self.checkpoint.keys()\n",
        "    def __iter__(self):\n",
        "        for key in self.checkpoint:\n",
        "            yield (key, self.__getitem__(key))\n",
        "    def __copy__(self):\n",
        "        return Checkpoint()\n",
        "    def copy(self):\n",
        "        return Checkpoint()\n",
        "\n",
        "model = GPTNeoForCausalLM.from_pretrained(pretrained_model_name_or_path=None, config=config, state_dict=Checkpoint())\n",
        "\n",
        "# Initialize the tokenizer and set up the bad_words_ids to exclude Author's Note tags\n",
        "tokenizer     = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "vocab         = tokenizer.get_vocab()\n",
        "vocab_keys    = vocab.keys()\n",
        "find_keys     = lambda char : [key for key in vocab_keys if key.find(char) != -1]\n",
        "bad_words     = []\n",
        "bad_words_ids = []\n",
        "\n",
        "bad_words.extend(find_keys(\"[\"))\n",
        "bad_words.extend(find_keys(\" [\"))\n",
        "bad_words.extend(find_keys(\"<|endoftext|>\"))\n",
        "for key in bad_words:\n",
        "  bad_id = vocab[key]\n",
        "  bad_words_ids.append([bad_id])\n",
        "\n",
        "clear_output()\n",
        "print(colored(\"DONE!\", \"green\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCHgJvfL4alW"
      },
      "source": [
        "#@title <b>Step 4 - Run Web Service</b> { display-mode: \"form\" }\n",
        "#@markdown Press the Play button. Flask will start and give you a \n",
        "#@markdown Cloudflare or Ngrok address which looks like this:<br/>\n",
        "#@markdown <i>https://\\<unique id\\>.trycloudflare.com/</i><br/>\n",
        "#@markdown <i>http://\\<unique id\\>.ngrok.io/</i><br/>\n",
        "#@markdown You will need to right-click this and copy the address.\n",
        "#@markdown Start the KoboldAI Client on your computer and choose \n",
        "#@markdown Google Colab as the model. You will be asked to paste \n",
        "#@markdown the address into the terminal.<br/><br/>\n",
        "#@markdown If your session is interrupted, you can just restart\n",
        "#@markdown this cell to get a new address without reinitializing\n",
        "#@markdown the model.</br></br>\n",
        "#@markdown <b>The first generation takes around a minute due to \n",
        "#@markdown compilation, but after that it should take less than \n",
        "#@markdown 10 seconds per sample.</b>\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "if connect_method == \"Cloudflare\":\n",
        "   run_with_cloudflared(app)\n",
        "elif connect_method == \"Ngrok\":\n",
        "   run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"<h1>KoboldAI Colab Service Running!</h1>\"\n",
        "\n",
        "@app.route('/request',methods = ['POST'])\n",
        "def koboldrequest():\n",
        "   if request.method == 'POST':\n",
        "      try:\n",
        "        clear_output()\n",
        "        js      = request.json\n",
        "        txt     = js[\"text\"]\n",
        "        min     = js[\"min\"]\n",
        "        max     = js[\"max\"]\n",
        "        rep_pen = js[\"rep_pen\"]\n",
        "        temp    = js[\"temperature\"]\n",
        "        top_p   = js[\"top_p\"]\n",
        "\n",
        "        # Compatability with un-updated clients\n",
        "        if(\"numseqs\" in js):\n",
        "          numseqs = js[\"numseqs\"]\n",
        "        else:\n",
        "          numseqs = 1\n",
        "\n",
        "        if(\"retfultxt\" in js):\n",
        "          retfultxt = js[\"retfultxt\"]\n",
        "        else:\n",
        "          retfultxt = True\n",
        "\n",
        "        print(colored(\"Received Data: {0}\".format(txt), \"yellow\"))\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        print(colored(\"Generating text, please wait...\", \"green\"))\n",
        "\n",
        "        tokens = tokenizer(txt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "        ids = tokens.cuda()\n",
        "\n",
        "        gen_tokens = model.generate(\n",
        "              ids.long().cuda(),\n",
        "              do_sample=True,\n",
        "              min_length=min,\n",
        "              max_length=max,\n",
        "              temperature=temp,\n",
        "              top_p = top_p,\n",
        "              repetition_penalty = rep_pen,\n",
        "              use_cache=True,\n",
        "              bad_words_ids=bad_words_ids,\n",
        "              num_return_sequences=numseqs\n",
        "          ).long()\n",
        "\n",
        "        genout = []\n",
        "        for tkns in gen_tokens:\n",
        "          if(not retfultxt):\n",
        "            # Strip context tokens out of returned sequences\n",
        "            dif = (len(tkns) - len(tokens[0])) * -1\n",
        "            tkns = tkns[dif:]\n",
        "          tkns = list(filter(lambda a: a != 50256, tkns))\n",
        "          genout.append(tokenizer.decode(tkns))\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if(len(genout) > 0 and genout[0] != \"\"):\n",
        "          if(retfultxt):\n",
        "            # Outdated client, send old JSON format\n",
        "            print(colored(\"Generated Text: {0}\".format(genout[0]), \"cyan\"))\n",
        "            response = app.response_class(\n",
        "              response=json.dumps({\"data\": {\"text\": genout[0]}}),\n",
        "              status=200,\n",
        "              mimetype='application/json'\n",
        "            )\n",
        "          else:\n",
        "            # New client format with numseq support\n",
        "            i = 0\n",
        "            for seq in genout:\n",
        "              print(colored(\"[Result {0}]\\n{1}\".format(i, seq), \"cyan\"))\n",
        "              i += 1\n",
        "            response = app.response_class(\n",
        "              response=json.dumps({\"data\": {\"seqs\": genout}}),\n",
        "              status=200,\n",
        "              mimetype='application/json'\n",
        "            )\n",
        "\n",
        "          return response\n",
        "        else:\n",
        "          print(colored(\"[ERROR] Something went wrong during generation!\", \"red\"))\n",
        "          response = app.response_class(\n",
        "            response=json.dumps({\"error\": {\"extensions\": {\"code\": \"Something went wrong during generation!\"}}}),\n",
        "            status=400,\n",
        "            mimetype='application/json'\n",
        "          )\n",
        "        \n",
        "        js         = {}\n",
        "        tokens     = []\n",
        "        ids        = []\n",
        "        gen_tokens = []\n",
        "        genout     = \"\"\n",
        "        response   = {}\n",
        "\n",
        "      except Exception as e:\n",
        "        print(colored(\"[ERROR] Something went wrong during generation!\", \"red\"))\n",
        "        print(colored(\"{0}\".format(e), \"red\"))\n",
        "        response = app.response_class(\n",
        "          response=json.dumps({\"error\": {\"extensions\": {\"code\": \"Something went wrong during generation! {0}\".format(e)}}}),\n",
        "          status=400,\n",
        "          mimetype='application/json'\n",
        "        )\n",
        "\n",
        "print(colored(\"Starup complete! Running web service.\", \"green\"))\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}