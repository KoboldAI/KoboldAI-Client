{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KoboldAI Server - GPTNeo-2.7B-Horni.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HflRYUWt8jo"
      },
      "source": [
        "**Welcome to the KoboldAI Colab Service!**<br/>\n",
        "*Note: This colab is intended to be used with the KoboldAI Client, [which can be downloaded from GitHub here](https://github.com/KoboldAI/KoboldAI-Client).*\n",
        "\n",
        "**Things you will need:**<br/>\n",
        "- A Google Drive account\n",
        "- One of the following models:\n",
        "   - The GPT-Neo-2.7B-Horni model from [mega](https://mega.nz/file/6BNykLJb#B6gxK3TnCKBpeOF1DJMXwaLc_gcTcqMS0Lhzr1SeJmc), [gdrive](https://drive.google.com/file/d/1-Jj_hlyNCQxuSnK7FFBXREGnRSMI5MoF/view?usp=sharing), or [torrent](https://tinyurl.com/pytorch-gptneo-horni)\n",
        "   - The GPT-Neo-2.7B-Horni-Ln model from [mega](https://mega.nz/file/rQcWCTZR#tCx3Ztf_PMe6OtfgI95KweFT5fFTcMm7Nx9Jly_0wpg), [gdrive](https://drive.google.com/file/d/1M1JY459RBIgLghtWDRDXlD4Z5DAjjMwg/view?usp=sharing), or [torrent](https://tinyurl.com/pytorch-gptneo-horni)\n",
        "\n",
        "**Preparation:**<br/>\n",
        "Download the model and place the .tar file in the root folder of your Google Drive. If you want to skip the unpacking stage of setup, extract the .tar to a folder in the root of your GDrive ***with the same name as the tar file.***<br/>\n",
        "(e.g. **gpt-neo-2.7B-horni.tar** > extract to folder named **gpt-neo-2.7B-horni**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5NcA61O-S02",
        "cellView": "form"
      },
      "source": [
        "#@title <b>Step 1 - Install Dependencies</b>\n",
        "#@markdown Press the Play button and wait for the script to finish.\n",
        "from IPython.display import clear_output\n",
        "from termcolor import colored\n",
        " \n",
        "!pip install flask-ngrok\n",
        "!pip install git+https://github.com/finetuneanon/transformers@gpt-neo-dungeon-localattention2\n",
        "!pip install termcolor\n",
        "!pip install flask_cloudflared\n",
        "clear_output()\n",
        "print(colored(\"DONE!\", \"green\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-6kNXRu2EQL",
        "cellView": "form"
      },
      "source": [
        "#@title <b>Step 2 - Adjust Your Settings</b>\n",
        "#@markdown 1. Choose which model you have downloaded:\n",
        "model_name = \"2.7B-horni\" #@param [\"2.7B-horni\", \"2.7B-horni-ln\", \"EleutherAI/gpt-neo-2.7B\"]\n",
        "#@markdown 2. Is it still archived (.tar) or did you extract it? (Ignore for stock Neo)\n",
        "is_archived = \"It's archived.\" #@param [\"It's archived.\", \"I extracted it.\"]\n",
        "#@markdown 3. Connect via Ngrok or Cloudflare?\n",
        "connect_method = \"Ngrok\" #@param [\"Ngrok\", \"Cloudflare\"]\n",
        "#@markdown 4. Run in half-precision mode? (Use half if you are getting CUDA out-of-memory errors.)\n",
        "precision = \"Half\" #@param [\"Half\", \"Full\"]\n",
        "#@markdown 5. Press Play button to lock in settings <b>(Do not skip!)</b>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aGzVuDR_eo1",
        "cellView": "form"
      },
      "source": [
        "#@title <b>Step 3 - Run Service</b>\n",
        "#@markdown Press the Play button. You will be asked to link your \n",
        "#@markdown Google Drive account to this colab. Click the link that \n",
        "#@markdown pops up, give Drive permission at the prompt, then copy \n",
        "#@markdown the access code that's presented back into this notebook.\n",
        "#@markdown <br/><br/>\n",
        "#@markdown This notebook will then extract the tar file (if necessary) \n",
        "#@markdown and initialize the AI model. <b>This will take several minutes.</b>\n",
        "#@markdown When the model is ready, Flask will start and give you a \n",
        "#@markdown Cloudflare or Ngrok address which looks like this:<br/>\n",
        "#@markdown <i>https://\\<unique id\\>.trycloudflare.com/</i><br/>\n",
        "#@markdown <i>http://\\<unique id\\>.ngrok.io/</i><br/>\n",
        "#@markdown You will need to right-click this and copy the address.\n",
        "#@markdown Start the KoboldAI Client on your computer and choose \n",
        "#@markdown Google Colab as the model. You will be asked to paste \n",
        "#@markdown the copied address into the terminal.\n",
        "from flask import Flask, redirect, url_for, request\n",
        "import json\n",
        "import torch\n",
        "import requests\n",
        "import subprocess\n",
        "from transformers import GPTNeoForCausalLM, AutoTokenizer, pipeline\n",
        "import tarfile\n",
        "from google.colab import drive\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from threading import Timer\n",
        "\n",
        "if connect_method == \"Cloudflare\":\n",
        "   from flask_cloudflared import run_with_cloudflared\n",
        "elif connect_method == \"Ngrok\":\n",
        "   from flask_ngrok import run_with_ngrok\n",
        "\n",
        "# Thanks to finetune for some of this startup code, I'm really not\n",
        "# familiar with the Colab environment\n",
        "model         = None\n",
        "tokenizer     = None\n",
        "custom_models = [\"2.7B-horni\", \"2.7B-horni-ln\"]\n",
        "is_archived   = True if is_archived == \"It's archived.\" else False\n",
        "\n",
        "# Get access to the unpacked model folder\n",
        "if model_name in custom_models:\n",
        "   # Get connected to Google Drive\n",
        "   print(colored(\"Requesting Google Drive access...\", \"magenta\"))\n",
        "   drive.mount('/content/drive/')\n",
        "   if is_archived:\n",
        "      # Archived. Set path to tar file and unpack it\n",
        "      model_gdrive = \"/content/drive/MyDrive/gpt-neo-{0}.tar\".format(model_name)\n",
        "      if not os.path.isdir(\"gpt-neo-\"+model_name):\n",
        "         print(colored(\"Unpacking tar file, please wait...\", \"magenta\"))\n",
        "         tar = tarfile.open(model_gdrive, \"r\")\n",
        "         tar.extractall()\n",
        "         tar.close()\n",
        "         print(colored(\"DONE!\", \"green\"))\n",
        "   else:\n",
        "      # Unpacked model already available, just set the path to it\n",
        "      model_gdrive = \"/content/drive/MyDrive/gpt-neo-{0}\".format(model_name)\n",
        "\n",
        "# Initialize the model\n",
        "print(colored(\"Initializing model, please wait...\", \"magenta\"))\n",
        "\n",
        "if model_name in custom_models:\n",
        "   if is_archived:\n",
        "      # If the model was archived, it now lives in the Colab's /content directory\n",
        "      checkpoint = torch.load(\"gpt-neo-\" + model_name + \"/pytorch_model.bin\", map_location=\"cuda:0\")\n",
        "      model = GPTNeoForCausalLM.from_pretrained(\"gpt-neo-\" + model_name, state_dict=checkpoint).half().to(\"cuda\").eval()\n",
        "   else:\n",
        "      # The unpacked folder lives on the user's GDrive\n",
        "      checkpoint = torch.load(model_gdrive + \"/pytorch_model.bin\", map_location=\"cuda:0\")\n",
        "      model = GPTNeoForCausalLM.from_pretrained(model_gdrive, state_dict=checkpoint).half().to(\"cuda\").eval()\n",
        "else:\n",
        "   from transformers.file_utils import cached_path, WEIGHTS_NAME, hf_bucket_url\n",
        "   archive_file = hf_bucket_url(model_name, filename=WEIGHTS_NAME)\n",
        "   resolved_archive_file = cached_path(archive_file)\n",
        "   checkpoint = torch.load(resolved_archive_file, map_location=\"cuda:0\")\n",
        "   for k in checkpoint.keys():\n",
        "      checkpoint[k] = checkpoint[k].half()\n",
        "   model = GPTNeoForCausalLM.from_pretrained(model_name, state_dict=checkpoint).half().to(\"cuda\").eval()\n",
        "\n",
        "for k in list(checkpoint.keys()):\n",
        "   del checkpoint[k]\n",
        "del checkpoint\n",
        "\n",
        "# Initialize the tokenizer and set up the bad_words_ids to exclude Author's Note tags\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "vocab         = tokenizer.get_vocab()\n",
        "vocab_keys    = vocab.keys()\n",
        "find_keys     = lambda char : [key for key in vocab_keys if key.find(char) != -1]\n",
        "bad_words     = []\n",
        "bad_words_ids = []\n",
        "\n",
        "bad_words.extend(find_keys(\"[\"))\n",
        "bad_words.extend(find_keys(\" [\"))\n",
        "for key in bad_words:\n",
        "  bad_id = vocab[key]\n",
        "  bad_words_ids.append([bad_id])\n",
        "\n",
        "# Enable 32-bit mode if the GPU can handle it\n",
        "if precision == \"Full\":\n",
        "  if torch.cuda.get_device_properties(0).total_memory > 15000 * 1024 * 1024:\n",
        "    print(colored(\"Big GPU detected, using fp32\", \"magenta\"))\n",
        "    model = model.float()\n",
        "\n",
        "print(colored(\"DONE!\", \"green\"))\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "if connect_method == \"Cloudflare\":\n",
        "   run_with_cloudflared(app)\n",
        "elif connect_method == \"Ngrok\":\n",
        "   run_with_ngrok(app)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"<h1>KoboldAI Colab Service Running!</h1>\"\n",
        "\n",
        "@app.route('/request',methods = ['POST'])\n",
        "def koboldrequest():\n",
        "   if request.method == 'POST':\n",
        "      try:\n",
        "        clear_output()\n",
        "        js      = request.json\n",
        "        txt     = js[\"text\"]\n",
        "        min     = js[\"min\"]\n",
        "        max     = js[\"max\"]\n",
        "        rep_pen = js[\"rep_pen\"]\n",
        "        temp    = js[\"temperature\"]\n",
        "        top_p   = js[\"top_p\"]\n",
        "\n",
        "        # Compatability with un-updated clients\n",
        "        if(\"numseqs\" in js):\n",
        "          numseqs = js[\"numseqs\"]\n",
        "        else:\n",
        "          numseqs = 1\n",
        "\n",
        "        if(\"retfultxt\" in js):\n",
        "          retfultxt = js[\"retfultxt\"]\n",
        "        else:\n",
        "          retfultxt = True\n",
        "\n",
        "        print(colored(\"Received Data: {0}\".format(txt), \"yellow\"))\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "        print(colored(\"Generating text, please wait...\", \"green\"))\n",
        "\n",
        "        tokens = tokenizer(txt, return_tensors=\"pt\").input_ids.to(\"cpu\")\n",
        "        ids = tokens.cuda()\n",
        "\n",
        "        gen_tokens = model.generate(\n",
        "              ids.long().cuda(),\n",
        "              do_sample=True,\n",
        "              min_length=min,\n",
        "              max_length=max,\n",
        "              temperature=temp,\n",
        "              top_p = top_p,\n",
        "              repetition_penalty = rep_pen,\n",
        "              use_cache=True,\n",
        "              bad_words_ids=bad_words_ids,\n",
        "              num_return_sequences=numseqs\n",
        "          ).long()\n",
        "\n",
        "        genout = []\n",
        "        for tkns in gen_tokens:\n",
        "          if(not retfultxt):\n",
        "            # Strip context tokens out of returned sequences\n",
        "            dif = (len(tkns) - len(tokens[0])) * -1\n",
        "            tkns = tkns[dif:]\n",
        "          genout.append(tokenizer.decode(tkns))\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        if(len(genout) > 0 and genout[0] != \"\"):\n",
        "          if(retfultxt):\n",
        "            # Outdated client, send old JSON format\n",
        "            print(colored(\"Generated Text: {0}\".format(genout[0]), \"cyan\"))\n",
        "            response = app.response_class(\n",
        "              response=json.dumps({\"data\": {\"text\": genout[0]}}),\n",
        "              status=200,\n",
        "              mimetype='application/json'\n",
        "            )\n",
        "          else:\n",
        "            # New client format with numseq support\n",
        "            i = 0\n",
        "            for seq in genout:\n",
        "              print(colored(\"[Result {0}]\\n{1}\".format(i, seq), \"cyan\"))\n",
        "              i += 1\n",
        "            response = app.response_class(\n",
        "              response=json.dumps({\"data\": {\"seqs\": genout}}),\n",
        "              status=200,\n",
        "              mimetype='application/json'\n",
        "            )\n",
        "\n",
        "          return response\n",
        "        else:\n",
        "          print(colored(\"[ERROR] Something went wrong during generation!\", \"red\"))\n",
        "          response = app.response_class(\n",
        "            response=json.dumps({\"error\": {\"extensions\": {\"code\": \"Something went wrong during generation!\"}}}),\n",
        "            status=400,\n",
        "            mimetype='application/json'\n",
        "          )\n",
        "        \n",
        "        js         = {}\n",
        "        tokens     = []\n",
        "        ids        = []\n",
        "        gen_tokens = []\n",
        "        genout     = \"\"\n",
        "        response   = {}\n",
        "\n",
        "      except Exception as e:\n",
        "        print(colored(\"[ERROR] Something went wrong during generation!\", \"red\"))\n",
        "        print(colored(\"{0}\".format(e), \"red\"))\n",
        "        response = app.response_class(\n",
        "          response=json.dumps({\"error\": {\"extensions\": {\"code\": \"Something went wrong during generation!\"}}}),\n",
        "          status=400,\n",
        "          mimetype='application/json'\n",
        "        )\n",
        "\n",
        "print(colored(\"Starup complete! Running web service.\", \"green\"))\n",
        "app.run()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}